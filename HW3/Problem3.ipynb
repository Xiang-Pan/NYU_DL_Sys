{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "Calculate the number of parameters in Alexnet. You will have to show calculations for each layer and then sum it to obtain the total number of parameters in Alexnet. When calculating you will need to account for all the filters (size, strides, padding) at each layer. Look at Sec. 3.5 and Figure 2 in Alexnet paper (see reference). Points will only be given when explicit calculations are shown for each layer. (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models.alexnet as alexnet\n",
    "model = alexnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The PyTorch Version Alexnet is different from the paper version!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "1429383808.0 61100840.0\n"
     ]
    }
   ],
   "source": [
    "from thop import profile\n",
    "input = torch.randn(1, 3, 224, 224)\n",
    "macs, params = profile(model, inputs=(input, ))\n",
    "print(macs, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "714691904.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    " * @Author: Xiang Pan\n",
    " * @Date: 2022-03-27 05:34:31\n",
    " * @LastEditTime: 2022-03-28 14:36:55\n",
    " * @LastEditors: Xiang Pan\n",
    " * @Description: \n",
    " * @FilePath: /HW3/Problem3.ipynb\n",
    " * @email: xiangpan@nyu.edu\n",
    "-->\n",
    "For Conv Layer:\n",
    "(n*m*l+1)*k\n",
    "n: width of the input feature map\n",
    "m: height of the input feature map\n",
    "l: channel of the input feature map\n",
    "k: width of the kernel\n",
    "\n",
    "Table for Alexnet Parameters Counts\n",
    "\n",
    "\n",
    "For FC Layer:\n",
    "(n+1)*k\n",
    "n: number of input neurons\n",
    "k: number of output neurons\n",
    "\n",
    "\n",
    "| Layer      | Filters | Kernel Size | Stride | Padding | Output Size | Parameter Count                  | Note                         |\n",
    "| ---------- | ------- | ----------- | ------ | ------- | ----------- | -------------------------------- | ---------------------------- |\n",
    "| Conv 1     | 96      | 11 * 11       | 4      |         | 55 * 55 * 96    | (11  *  11  *  3  *  96) + 96 = 34848  |                              |\n",
    "| Max Pool 1 | -       | 3 * 3         | 2      |         | 27 * 27 * 96    |                                  |                              |\n",
    "| Conv2      | 128     | 5 * 5         | 1      | 2       | 27 * 27 * 256   | ((5 * 5 * 48 * 128) + 128) * 2 = 307456  | Two GPU                      |\n",
    "| Max Pool 2 | -       | 3 * 3         | 2      |         | 13 * 13 * 256   |                                  |                              |\n",
    "| Conv3      | 384     | 3 * 3         | 1      | 1       | 13 * 13 * 384   | 3 * 3 * 256 * 384+384 = 885120         | Cross Connection             |\n",
    "| Conv4      | 192 * 2   | 3 * 3         | 1      | 1       | 13 * 13 * 384   | (3 * 3 * 192 * 192+192) * 2 = 663936     | Only Connect to the Same GPU |\n",
    "| Conv5      | 128 * 2   | 3 * 3         | 1      | 1       | 13 * 13 * 256   | (3 * 3 * 192 * 128+128) * 2 = 442624     | Only Connect to the Same GPU |\n",
    "| Max Pool 3 |         | 3 * 3         | 2      |         |             |                                  |                              |\n",
    "| FC6        |         |             |        |         |             | (6 * 6 * 128 * 2) * 4096+4096 = 37752832 |                              |\n",
    "| FC7        |         |             |        |         |             | 4096 * 4096 + 4096= 16781312       |                              |\n",
    "| FC8        |         |             |        |         |             | 4096 * 1000 + 1000 = 4097000       |                              |\n",
    "\n",
    "\n",
    "**Total Number**\n",
    "\n",
    "\n",
    "34848 + 307456 + 885120 + 663936 + 442624 + 37752832 + 16781312 + 4097000 = 60965128\n",
    "\n",
    "\n",
    "Something we need to note there is that, for the original paper, there are two parts of the network in two different GPUs, so the Connection between that two parts is corssed in conv3 but seperated in conv4 and conv5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "VGG (Simonyan et al.) has an extremely homogeneous architecture that only performs 3x3 convolutions with stride 1 and pad 1 and 2x2 max pooling with stride 2 (and no padding) from the beginning to the end. However VGGNet is very expensive to evaluate and uses a lot more memory and parameters. Refer to VGG19 architecture on page 3 in Table 1 of the paper by Simonyan et al. You need to complete Table 1 below for calculating activation units and parameters at each layer in VGG19 (without counting biases). Its been partially filled for you. (6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.vgg import vgg19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg19(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give the source table and finished table below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Layer ID | Type     | Patch Size (Height) | Patch Size (Width) | Stride (Height) | Stride (Width) | Output Size (Height) | Output Size (Width) | Output Size (Channel) | Memory     | Params      | Ratio  |\n",
    "| -------- | -------- | ------------------- | ------------------ | --------------- | -------------- | -------------------- | ------------------- | --------------------- | ---------- | ----------- | ------ |\n",
    "| 0        | Input    |                     |                    |                 |                | 224                  | 224                 | 3                     | 150,528    |             |        |\n",
    "| 1        | Conv1\\_1 | 3                   | 3                  | 1               | 1              | 224                  | 224                 | 64                    | 3,211,264  | 1,728       | 0.00%  |\n",
    "| 2        | Conv1\\_2 | 3                   | 3                  | 1               | 1              | 224                  | 224                 | 64                    | 3,211,264  | 36,864      | 0.03%  |\n",
    "| 3        | Pool1    | 2                   | 2                  | 2               | 2              | 112                  | 112                 | 64                    | 802,816    |             |        |\n",
    "| 4        | Conv2\\_1 | 3                   | 3                  | 1               | 1              | 112                  | 112                 | 128                   | 1,605,632  | 73,728      | 0.05%  |\n",
    "| 5        | Conv2\\_2 | 3                   | 3                  | 1               | 1              | 112                  | 112                 | 128                   | 1,605,632  | 147,456     | 0.10%  |\n",
    "| 6        | Pool2    | 2                   | 2                  | 2               | 2              | 56                   | 56                  | 128                   | 401,408    |             |        |\n",
    "| 7        | Conv3\\_1 | 3                   | 3                  | 1               | 1              | 56                   | 56                  | 256                   | 802,816    | 294,912     | 0.21%  |\n",
    "| 8        | Conv3\\_2 | 3                   | 3                  | 1               | 1              | 56                   | 56                  | 256                   | 802,816    | 589,824     | 0.41%  |\n",
    "| 9        | Conv3\\_3 | 3                   | 3                  | 1               | 1              | 56                   | 56                  | 256                   | 802,816    | 589,824     | 0.41%  |\n",
    "| 10       | Conv3\\_4 | 3                   | 3                  | 1               | 1              | 56                   | 56                  | 256                   | 802,816    | 589,824     | 0.41%  |\n",
    "| 11       | Pool3    | 2                   | 2                  | 2               | 2              | 28                   | 28                  | 256                   | 200,704    |             |        |\n",
    "| 12       | Conv4\\_1 | 3                   | 3                  | 1               | 1              | 28                   | 28                  | 512                   | 401,408    | 1,179,648   | 0.82%  |\n",
    "| 13       | Conv4\\_2 | 3                   | 3                  | 1               | 1              | 28                   | 28                  | 512                   | 401,408    | 2,359,296   | 1.64%  |\n",
    "| 14       | Conv4\\_3 | 3                   | 3                  | 1               | 1              | 28                   | 28                  | 512                   | 401,408    | 2,359,296   | 1.64%  |\n",
    "| 15       | Conv4\\_4 | 3                   | 3                  | 1               | 1              | 28                   | 28                  | 512                   | 401,408    | 2,359,296   | 1.64%  |\n",
    "| 16       | Pool4    | 2                   | 2                  | 2               | 2              | 14                   | 14                  | 512                   | 100,352    |             |        |\n",
    "| 17       | Pool5\\_1 | 3                   | 3                  | 1               | 1              | 14                   | 14                  | 512                   | 100,352    | 2,359,296   | 1.64%  |\n",
    "| 18       | Pool5\\_2 | 3                   | 3                  | 1               | 1              | 14                   | 14                  | 512                   | 100,352    | 2,359,296   | 1.64%  |\n",
    "| 19       | Pool5\\_3 | 3                   | 3                  | 1               | 1              | 14                   | 14                  | 512                   | 100,352    | 2,359,296   | 1.64%  |\n",
    "| 20       | Pool5\\_4 | 3                   | 3                  | 1               | 1              | 14                   | 14                  | 512                   | 100,352    | 2,359,296   | 1.64%  |\n",
    "| 21       | Pool6    | 2                   | 2                  | 2               | 2              | 7                    | 7                   | 512                   | 25,088     |             |        |\n",
    "| 22       | FC1      |                     |                    |                 |                | 1                    | 1                   | 4096                  | 4,096      | 102,760,448 | 71.53% |\n",
    "| 23       | FC2      |                     |                    |                 |                | 1                    | 1                   | 4096                  | 4,096      | 16,777,216  | 11.68% |\n",
    "| 24       | FC3      |                     |                    |                 |                | 1                    | 1                   | 1000                  | 1,000      | 4,096,000   | 2.85%  |\n",
    "| Total    |          |                     |                    |                 |                |                      |                     |                       | 16,542,184 | 143,652,544 |        |\n",
    "\n",
    "\n",
    "VGG19\n",
    "\n",
    "| Layer     | Number of Activations (Memory) | Parameters (Compute)       |\n",
    "| --------- | ------------------------------ | -------------------------- |\n",
    "| Input     | 224\\*224\\*3=150K               | 0                          |\n",
    "| CONV3-64  | 224\\*224\\*64=3.2M              | (3\\*3\\*3)\\*64=1,728        |\n",
    "| CONV3-64  | 224\\*224\\*64=3.2M              | (3\\*3\\*64)\\*64=36,864      |\n",
    "| POOL2     | 112\\*112\\*64=800K              |                            |\n",
    "| CONV3-128 | 112\\*112\\*128=1568K            | (3\\*3\\*64)\\*128=73728      |\n",
    "| CONV3-128 | 112\\*112\\*128==1568K           | (3\\*3\\*128)\\*128=147456    |\n",
    "| POOL2     | 56\\*56\\*128=400K               | 0                          |\n",
    "| CONV3-256 | 56\\*56\\*256=800K               | (3\\*3\\*128)\\*256=294912    |\n",
    "| CONV3-256 | 56\\*56\\*256=800K               | (3\\*3\\*256)\\*256=589824    |\n",
    "| CONV3-256 | 56\\*56\\*256=800K               | (3\\*3\\*256)\\*256=589824    |\n",
    "| CONV3-256 | 56\\*56\\*256=800K               | (3\\*3\\*256)\\*256=589824    |\n",
    "| POOL2     | 56\\*56\\*128=401408             | 0                          |\n",
    "| CONV3-512 | 28\\*28\\*512=400K               | (3\\*3\\*256)\\*512=1,179,648 |\n",
    "| CONV3-512 | 28\\*28\\*512=400K               | (3\\*3\\*512)\\*512=2359296   |\n",
    "| CONV3-512 | 28\\*28\\*512=400K               | (3\\*3\\*512)\\*512=2359296   |\n",
    "| CONV3-512 | 28\\*28\\*512=400K               | (3\\*3\\*512)\\*512=2359296   |\n",
    "| POOL2     | 28\\*28\\*256=200,704            | 0                          |\n",
    "| CONV3-512 | 14\\*14\\*512=100k               | (3\\*3\\*512)\\*512=2359296   |\n",
    "| CONV3-512 | 14\\*14\\*512=100k               | (3\\*3\\*512)\\*512=2359296   |\n",
    "| CONV3-512 | 14\\*14\\*512=100k               | (3\\*3\\*512)\\*512=2359296   |\n",
    "| CONV3-512 | 14\\*14\\*512=100k               | (3\\*3\\*512)\\*512=2359296   |\n",
    "| POOL2     | 7\\*7\\*512=25088                | 0                          |\n",
    "| FC        | 4096                           | 25088\\*4096=102,760,448    |\n",
    "| FC        | 4096                           | 4096\\*4096=16,777,216      |\n",
    "| FC        | 1000                           |                            |\n",
    "| TOTAL     | 16,542,184                     | 143,652,544                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    "VGG architectures have smaller filters but deeper networks compared to Alexnet (3x3 compared to 11x11 or 5x5). Show that a stack of N convolution layers each of filter size F × F has the same receptive field as one convolution layer with filter of size (NF − N + 1) × (NF − N + 1). Use this to calculate the receptive field of 3 filters of size 5x5. (4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have\n",
    "\n",
    "\\begin{equation}\n",
    "R F_{i+1}=R F_{i}+(k-1) \\times S_{i}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "S_{i}=\\prod_{i=1}^{i} \\text { Stride }_{i}\n",
    "\\end{equation}\n",
    "\n",
    "$RF_{i+1}$ is the current receptive field and $RF_{i}$ is the previous receptive field. k is the kernel size and $S_{i}$ is the product of previous strides.\n",
    "\n",
    "\n",
    "The stacked 3 filter convolution layer has the same receptive field as the single filter convolution layer with a filter size of (5\\*3-3+1)x(5\\*3-3+1) = (13 x 13)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\n",
    "The original Googlenet paper (Szegedy et al.) proposes two architectures for Inception module, shown in Figure 2 on page 5 of the paper, referred to as naive and dimensionality reduction respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    " * @Author: Xiang Pan\n",
    " * @Date: 2022-03-27 05:34:31\n",
    " * @LastEditTime: 2022-03-27 09:17:50\n",
    " * @LastEditors: Xiang Pan\n",
    " * @Description: \n",
    " * @FilePath: /HW3/Problem3.ipynb\n",
    " * @email: xiangpan@nyu.edu\n",
    "-->\n",
    "### (a)\n",
    "What is the general idea behind designing an inception module (parallel convolutional filters of different sizes with a pooling followed by concatenation) in a convolutional neural network ? (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-branch structure**: Each inception module is divided into four branches, and concatenation is performed on the output of all branches when outputting. 1*1conv makes the caculation faster.\n",
    "\n",
    "**Heterogeneous branch structure**: the structure of each branch is different, mainly in the depth and kernel size of the branch. Objects of the same category may have different sizes in different pictures, so features generated by kernels of different sizes should be integrated within the same module, which is beneficial for CNN to identify objects of different sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "Assuming the input to inception module (referred to as ”previous layer” in Figure 2 of the paper) has size 32x32x256, calculate the output size after filter concatenation for the naive and dimensionality reduction inception architectures with number of filters given in Figure 1. (4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Width=100](./problem3/inception_module.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 224, 224]) torch.Size([1, 192, 224, 224]) torch.Size([1, 96, 224, 224]) torch.Size([1, 256, 224, 224])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 672, 224, 224])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NaiveInception(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1x1 = nn.Conv2d(256, 128, kernel_size=1)\n",
    "        self.conv3x3 = nn.Conv2d(256, 192, kernel_size=3, padding=1)\n",
    "        self.conv5x5 = nn.Conv2d(256, 96, kernel_size=5, padding=2)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1x1(x)\n",
    "        x2 = self.conv3x3(x)\n",
    "        x3 = self.conv5x5(x)\n",
    "        x4 = self.maxpool(x)\n",
    "        print(x1.shape, x2.shape, x3.shape, x4.shape)\n",
    "        return torch.cat([x1, x2, x3, x4], dim=1)\n",
    "\n",
    "model = NaiveInception()\n",
    "model(torch.randn(1, 256, 224, 224)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape is (672, 224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "Next calculate the total number of convolutional operations for each of the two inception architecture again assuming the input to the module has dimensions 32x32x256 and number of filters given in Figure 1. (4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 224, 224]) torch.Size([1, 192, 224, 224]) torch.Size([1, 96, 224, 224]) torch.Size([1, 64, 224, 224])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 480, 224, 224])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class InceptionWithDimReduction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1x1 = nn.Sequential(nn.Conv2d(256, 128, kernel_size=1))\n",
    "        self.conv3x3 = nn.Sequential(nn.Conv2d(256, 128, kernel_size=1),\n",
    "                                        nn.Conv2d(128, 192, kernel_size=3, padding=1))\n",
    "        self.conv5x5 = nn.Sequential(nn.Conv2d(256, 32, kernel_size=1),\n",
    "                                        nn.Conv2d(32, 96, kernel_size=5, padding=2))\n",
    "        self.poolbranch = nn.Sequential(nn.MaxPool2d(kernel_size=3, stride=1),\n",
    "                                        nn.Conv2d(256, 64, kernel_size=1, padding=1))\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1x1(x)\n",
    "        x2 = self.conv3x3(x)\n",
    "        x3 = self.conv5x5(x)\n",
    "        x4 = self.poolbranch(x)\n",
    "        print(x1.shape, x2.shape, x3.shape, x4.shape)\n",
    "        return torch.cat([x1, x2, x3, x4], dim=1)\n",
    "model = InceptionWithDimReduction()\n",
    "model(torch.randn(1, 256, 224, 224)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **NaiveInception**\n",
    "\n",
    "Conv Ops for **NaiveInception**:\n",
    "\n",
    "[conv1x1 branch, conv1x1 128] 128 * 224 * 224 * 1 * 1 * 256 = 1644167168\n",
    "[conv3x3 branch, conv3x3 192] 192 * 224 * 224 * 3 * 3 * 256 = 22196256768\n",
    "[conv5x5 branch, conv5x5 96] 96 * 224 * 224 * 5 * 5 * 256 = 30828134400\n",
    "\n",
    "Total Conv Ops for **NaiveInception**: 1644167168 + 22196256768 + 30828134400 = 54668558336"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **InceptionWithDimReduction** output is (480, 224, 224). 224\\*224\\*480.\n",
    "\n",
    "Conv Ops for InceptionWithDimReduction:\n",
    "\n",
    "\n",
    "[conv1x1 branch, conv1x1 128] 224 * 224 * 128 * 1 * 1 * 256 = 1644167168\n",
    "\n",
    "[conv3x3 branch, conv1x1 128] 224 * 224 * 128 * 1 * 1 * 256 = 1644167168 \n",
    "\n",
    "[conv3x3 branch, conv3x3 192] 224 * 224 * 192 * 3 * 3 * 128 = 11098128384 \n",
    "\n",
    "[conv5x5 branch, conv1x1 32] 224 * 224 * 32 * 1 * 1 * 256 = 411041792 \n",
    "\n",
    "[conv5x5 branch, conv5x5 96] 224 * 224 * 96 * 5 * 5 * 32 = 3853516800\n",
    "\n",
    "Total Conv Ops for InceptionWithDimReduction: 1644167168 + 1644167168 + 11098128384 + 411041792 + 3853516800 = 18651021312"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)\n",
    "Based on the calculations in part (c) explain the problem with naive architecture and how dimensionality reduction architecture helps (Hint: compare computational complexity). How much is the computational saving ? (2+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimension reduction architecture reduces the dimensionality for each branch, thus the complex convolutional operations with less input dimension, the total convolutional operations are reduced.\n",
    "\n",
    "The naive architecture has a total of 54668558336 convolutional operations. The dimensionality reduction architecture has a total of 18651021312 convolutional operations. The dimensionality reduction architecture reduces the number of convolutional operations by a factor of 3 \n",
    "\n",
    "\\begin{equation}\n",
    "\\text{ops-naive} \\approx 3* \\text{ops-DimReduction}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\n",
    "Faster-RCNN is a CNN based architecture for object detection which is much faster that Fast-RCNN.\n",
    "\n",
    "Read about Faster-RCNN in Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks and answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "What is the main difference between Fast-RCNN and Faster-RCNN that resulted in faster detection using Faster-RCNN? (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fast-RCNN: selective search for region selection, which runs in CPU and slow (but fast than multi-stage RCNN).\n",
    "\n",
    "Faster-RCNN: Rigion Proposal Network (RPN) for region selection, CNN for feature extraction, and RoI pooling for object detection. \n",
    "\n",
    "The main difference is using RPN for region selection and RPN shared CNN feature mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "What is Region Proposal Network (RPN)? Clearly explain its architecture. (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input of RPN is a feature map, and the output has two branches, one is the classification (cls) layer, which is used to determine whether there is a target, and the other is the regression (reg) layer, which is used to generate a series of target proposal boxes. \n",
    "\n",
    "In RPN, we will use an nxn sliding window to traverse the entire feature map. Each sliding window will generate K different scales and different proportions of proposed regions, which we call Anchors. Each An anchor will generate 2k scores (target and non-target) in the clf layer, and 4k position information in the reg layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "Explain how are region proposals generated from RPN using an example image.(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the figure 3 from Faster-RCNN paper to illustrate the RPN. CNN maps the image to a feature map, and then we use the feature map to generate a series of anchors. Each anchor is a sliding window with a different scale and different aspect ratio. The anchors are then used to generate a series of target and non-target proposals with coordinates difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./problem3/Faster_RCNN_fig3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have k types of anchors, each with different scales and different ratios. The sliding window will traverse the entire feature map, and for each sliding window, we will generate k anchors. For each anchor, we will generate 2k scores (target and non-target) in the clf layer, and 4k position information in the reg layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4)\n",
    "There is a lot of overlap in the region proposals generated by RPN. What technique is used in Faster-RCNN to reduce the number of proposals to roughly 2000? Explain how does this technique work using an example. (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Ignore Cross-boundary anchors**:\n",
    "During training, they ignore all cross-boundary anchors so they do not contribute to the loss.\n",
    "\n",
    "<!-- ![width=20](./problem3/cross-boundary.png) -->\n",
    "<img src=\"./problem3/cross-boundary.png\" width=200>\n",
    "\n",
    "The red anchor is ignored.\n",
    "\n",
    "**Non-maximum suppression(NMS)**:\n",
    "Some RPN proposals highly overlap with each other. To reduce redundancy, they adopt non-maximum suppression (NMS) on the proposal regions based on their cls scores. They ﬁx the IoU threshold for NMS at 0.7, which leaves about 2000 proposal regions per image. \n",
    "\n",
    "<img src=\"./problem3/NMS.png\" width=200>\n",
    "\n",
    "We see that red and green anchors have greater area of intersection among them , and IoU for green anchor is less than that of red, hence the green anchor can be ignored. This mechanism is known as NMS.\n",
    "<!-- As we will show, NMS does not harm the ultimate detection accuracy, but substantially reduces the number of proposals. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6af05ba0047cd36248220b2bd1e90eef247fc9b76c139bbd2c6e6e388383ae6c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
