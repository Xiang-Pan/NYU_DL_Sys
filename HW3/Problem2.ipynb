{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problme 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to experiment with PyTorch’s DataParallel Module, which is PyTorch’s Synchronous SGD implementation across a number of GPUs on the same server. In particular, we will train ResNet-18 implementation from https://github.com/kuangliu/pytorch-cifar with num workers=2, running up to 4 GPUs with DataParallel (DP) Module. Use SGD optimizers with 0.1 as the learning rate, momentum 0.9, weight decay 5e-4. For this question, you need to do experiment with multiple GPUs on the same server. You may need to execute this on NYU Greene Cluster.\n",
    "\n",
    "Create a PyTorch program with a DataLoader that loads the images and the related labels from torchvision CIFAR10 dataset. Import CIFAR10 dataset for the torchvision package, with the following sequence of transformations:\n",
    "\n",
    "• Random cropping, with size 32x32 and padding 4\n",
    "\n",
    "• Random horizontal flipping with a probability 0.5\n",
    "\n",
    "• Normalize each image’s RGB channel with mean(0.4914, 0.4822, 0.4465) and variance (0.2023, 0.1994, 0.2010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataLoader for the training set uses a minibatch size of 128 and 3 IO processes (i.e., num workers=2). The DataLoader for the testing set uses minibatch size of 100 and 3 IO processes (i.e., num workers =2). Create a main function that creates the DataLoaders for the training set and the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "Measure how long does it take to compete 1 epoch of training using different batch size on a single GPU. Start from batch size 32, increase by 4-fold for each measurement (i.e., 32, 128, 512 ...) until single GPU memory cannot hold the batch size. For each run, run 2 epochs, the first epoch is used to warmup CPU/GPU cache; and you should report the training time (excluding data I/O; but including data movement from CPU to GPU, gradients calculation and weights update) based on the 2nd epoch training. (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),#random crop\n",
    "                                      transforms.RandomHorizontalFlip(0.5),#random flip\n",
    "                                      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),#normalize\n",
    "                                      transforms.ToTensor(),])#convert to tensor\n",
    "train_dataset = datasets.CIFAR10(\"./cached_datasets/CIFAR10\", train=True, download=True, transform=train_transform)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please refer to the code in problem2 folder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"./problem2/2_1.png\" width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the straight line to calculate the time for each configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"./problem2/2_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Relative Time (Wall)</th>\n",
       "      <th>SGD_dp_batch_size_8192_4gpus - _step</th>\n",
       "      <th>SGD_dp_batch_size_8192_4gpus - _step__MIN</th>\n",
       "      <th>SGD_dp_batch_size_8192_4gpus - _step__MAX</th>\n",
       "      <th>SGD_dp_batch_size_8192_4gpus - _timestamp</th>\n",
       "      <th>SGD_dp_batch_size_8192_4gpus - _timestamp__MIN</th>\n",
       "      <th>SGD_dp_batch_size_8192_4gpus - _timestamp__MAX</th>\n",
       "      <th>SGD_dp_batch_size_8192_4gpus - _runtime</th>\n",
       "      <th>SGD_dp_batch_size_8192_4gpus - _runtime__MIN</th>\n",
       "      <th>SGD_dp_batch_size_8192_4gpus - _runtime__MAX</th>\n",
       "      <th>...</th>\n",
       "      <th>SGD_dp_batch_size_8192_2gpus - _step__MAX</th>\n",
       "      <th>SGD_dp_batch_size_8192_2gpus - _timestamp</th>\n",
       "      <th>SGD_dp_batch_size_8192_2gpus - _timestamp__MIN</th>\n",
       "      <th>SGD_dp_batch_size_8192_2gpus - _timestamp__MAX</th>\n",
       "      <th>SGD_dp_batch_size_8192_2gpus - _runtime</th>\n",
       "      <th>SGD_dp_batch_size_8192_2gpus - _runtime__MIN</th>\n",
       "      <th>SGD_dp_batch_size_8192_2gpus - _runtime__MAX</th>\n",
       "      <th>SGD_dp_batch_size_8192_2gpus - epoch</th>\n",
       "      <th>SGD_dp_batch_size_8192_2gpus - epoch__MIN</th>\n",
       "      <th>SGD_dp_batch_size_8192_2gpus - epoch__MAX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1648372577</td>\n",
       "      <td>1648372568</td>\n",
       "      <td>1648372586</td>\n",
       "      <td>49</td>\n",
       "      <td>40</td>\n",
       "      <td>58</td>\n",
       "      <td>...</td>\n",
       "      <td>15</td>\n",
       "      <td>1.648371e+09</td>\n",
       "      <td>1648371067</td>\n",
       "      <td>1648371088</td>\n",
       "      <td>49.875</td>\n",
       "      <td>39</td>\n",
       "      <td>60</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Relative Time (Wall)  SGD_dp_batch_size_8192_4gpus - _step  \\\n",
       "0                   NaN                                   7.5   \n",
       "\n",
       "   SGD_dp_batch_size_8192_4gpus - _step__MIN  \\\n",
       "0                                          0   \n",
       "\n",
       "   SGD_dp_batch_size_8192_4gpus - _step__MAX  \\\n",
       "0                                         15   \n",
       "\n",
       "   SGD_dp_batch_size_8192_4gpus - _timestamp  \\\n",
       "0                                 1648372577   \n",
       "\n",
       "   SGD_dp_batch_size_8192_4gpus - _timestamp__MIN  \\\n",
       "0                                      1648372568   \n",
       "\n",
       "   SGD_dp_batch_size_8192_4gpus - _timestamp__MAX  \\\n",
       "0                                      1648372586   \n",
       "\n",
       "   SGD_dp_batch_size_8192_4gpus - _runtime  \\\n",
       "0                                       49   \n",
       "\n",
       "   SGD_dp_batch_size_8192_4gpus - _runtime__MIN  \\\n",
       "0                                            40   \n",
       "\n",
       "   SGD_dp_batch_size_8192_4gpus - _runtime__MAX  ...  \\\n",
       "0                                            58  ...   \n",
       "\n",
       "   SGD_dp_batch_size_8192_2gpus - _step__MAX  \\\n",
       "0                                         15   \n",
       "\n",
       "   SGD_dp_batch_size_8192_2gpus - _timestamp  \\\n",
       "0                               1.648371e+09   \n",
       "\n",
       "   SGD_dp_batch_size_8192_2gpus - _timestamp__MIN  \\\n",
       "0                                      1648371067   \n",
       "\n",
       "   SGD_dp_batch_size_8192_2gpus - _timestamp__MAX  \\\n",
       "0                                      1648371088   \n",
       "\n",
       "   SGD_dp_batch_size_8192_2gpus - _runtime  \\\n",
       "0                                   49.875   \n",
       "\n",
       "   SGD_dp_batch_size_8192_2gpus - _runtime__MIN  \\\n",
       "0                                            39   \n",
       "\n",
       "   SGD_dp_batch_size_8192_2gpus - _runtime__MAX  \\\n",
       "0                                            60   \n",
       "\n",
       "   SGD_dp_batch_size_8192_2gpus - epoch  \\\n",
       "0                                 0.625   \n",
       "\n",
       "   SGD_dp_batch_size_8192_2gpus - epoch__MIN  \\\n",
       "0                                          0   \n",
       "\n",
       "   SGD_dp_batch_size_8192_2gpus - epoch__MAX  \n",
       "0                                          2  \n",
       "\n",
       "[1 rows x 121 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "api = wandb.Api()\n",
    "\n",
    "# Project is specified by <entity/project-name>\n",
    "runs = api.runs(\"xiang-pan/NYU_DL_Sys-HW3_problem2\")\n",
    "summary_list = [] \n",
    "config_list = [] \n",
    "name_list = [] \n",
    "res = {}\n",
    "for run in runs: \n",
    "    data = run.history()\n",
    "    log_name = run.name\n",
    "    res[log_name] = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['SGD_dp_batch_size_512_2gpus', 'SGD_dp_batch_size_2048_2gpus', 'SGD_dp_batch_size_8192_4gpus', 'SGD_dp_batch_size_2048_4gpus', 'SGD_dp_batch_size_512_4gpus', 'SGD_dp_batch_size_128_4gpus', 'SGD_dp_batch_size_32_4gpus', 'SGD_dp_batch_size_8192_1gpus', 'SGD_dp_batch_size_2048_1gpus', 'SGD_dp_batch_size_512_1gpus', 'SGD_dp_batch_size_128_1gpus', 'SGD_dp_batch_size_32_1gpus', 'SGD_dp_batch_size_8192_2gpus', 'SGD_dp_batch_size_128_2gpus', 'SGD_dp_batch_size_32_2gpus'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SGD_dp_batch_size_512_2gpus':      trainer/global_step  _step  _runtime  train_loss_step  epoch  _timestamp  \\\n",
       " 0                      0      0        18         2.420080      0  1649128380   \n",
       " 1                      1      1        19         2.769084      0  1649128381   \n",
       " 2                      2      2        19         3.562219      0  1649128381   \n",
       " 3                      3      3        19         3.776740      0  1649128381   \n",
       " 4                      4      4        19         3.318298      0  1649128381   \n",
       " ..                   ...    ...       ...              ...    ...         ...   \n",
       " 193                  192    193        37         1.172103      1  1649128399   \n",
       " 194                  193    194        37         1.230016      1  1649128399   \n",
       " 195                  194    195        37         1.342752      1  1649128399   \n",
       " 196                  195    196        37         1.132757      1  1649128399   \n",
       " 197                  195    197        38              NaN      2  1649128400   \n",
       " \n",
       "      train_loss_epoch  \n",
       " 0                 NaN  \n",
       " 1                 NaN  \n",
       " 2                 NaN  \n",
       " 3                 NaN  \n",
       " 4                 NaN  \n",
       " ..                ...  \n",
       " 193               NaN  \n",
       " 194               NaN  \n",
       " 195               NaN  \n",
       " 196               NaN  \n",
       " 197          1.342205  \n",
       " \n",
       " [198 rows x 7 columns],\n",
       " 'SGD_dp_batch_size_2048_2gpus':     trainer/global_step  _step  _runtime  train_loss_step  epoch  _timestamp  \\\n",
       " 0                     0      0        20         2.407045      0  1649128132   \n",
       " 1                     1      1        20         2.558295      0  1649128132   \n",
       " 2                     2      2        20         2.766725      0  1649128132   \n",
       " 3                     3      3        20         3.660211      0  1649128132   \n",
       " 4                     4      4        21         4.084877      0  1649128133   \n",
       " 5                     5      5        21         3.603554      0  1649128133   \n",
       " 6                     6      6        21         3.600378      0  1649128133   \n",
       " 7                     7      7        21         3.024597      0  1649128133   \n",
       " 8                     8      8        22         2.548291      0  1649128134   \n",
       " 9                     9      9        22         3.080903      0  1649128134   \n",
       " 10                   10     10        22         2.824992      0  1649128134   \n",
       " 11                   11     11        23         2.455785      0  1649128135   \n",
       " 12                   12     12        23         2.284315      0  1649128135   \n",
       " 13                   13     13        23         2.387378      0  1649128135   \n",
       " 14                   14     14        24         2.258087      0  1649128136   \n",
       " 15                   15     15        24         2.535852      0  1649128136   \n",
       " 16                   16     16        24         2.237878      0  1649128136   \n",
       " 17                   17     17        25         2.035352      0  1649128137   \n",
       " 18                   18     18        25         2.010104      0  1649128137   \n",
       " 19                   19     19        25         2.214593      0  1649128137   \n",
       " 20                   20     20        26         2.089128      0  1649128138   \n",
       " 21                   21     21        26         1.944005      0  1649128138   \n",
       " 22                   22     22        26         1.937898      0  1649128138   \n",
       " 23                   23     23        27         2.004253      0  1649128139   \n",
       " 24                   24     24        28         2.022934      0  1649128140   \n",
       " 25                   24     25        28              NaN      1  1649128140   \n",
       " 26                   25     26        30         1.917986      1  1649128142   \n",
       " 27                   26     27        30         2.046042      1  1649128142   \n",
       " 28                   27     28        30         1.983665      1  1649128142   \n",
       " 29                   28     29        31         1.961981      1  1649128143   \n",
       " 30                   29     30        31         1.930356      1  1649128143   \n",
       " 31                   30     31        31         1.869945      1  1649128143   \n",
       " 32                   31     32        32         1.867186      1  1649128144   \n",
       " 33                   32     33        32         1.846186      1  1649128144   \n",
       " 34                   33     34        32         1.829378      1  1649128144   \n",
       " 35                   34     35        33         1.847615      1  1649128145   \n",
       " 36                   35     36        33         1.775432      1  1649128145   \n",
       " 37                   36     37        33         1.822025      1  1649128145   \n",
       " 38                   37     38        34         1.789850      1  1649128146   \n",
       " 39                   38     39        34         1.758589      1  1649128146   \n",
       " 40                   39     40        34         1.721828      1  1649128146   \n",
       " 41                   40     41        35         1.739697      1  1649128147   \n",
       " 42                   41     42        35         1.705283      1  1649128147   \n",
       " 43                   42     43        35         1.711573      1  1649128147   \n",
       " 44                   43     44        36         1.695738      1  1649128148   \n",
       " 45                   44     45        36         1.679440      1  1649128148   \n",
       " 46                   45     46        37         1.677661      1  1649128149   \n",
       " 47                   46     47        37         1.728877      1  1649128149   \n",
       " 48                   47     48        37         1.725134      1  1649128149   \n",
       " 49                   48     49        37         1.675917      1  1649128149   \n",
       " 50                   49     50        38         1.621064      1  1649128150   \n",
       " 51                   49     51        38              NaN      2  1649128150   \n",
       " \n",
       "     train_loss_epoch  \n",
       " 0                NaN  \n",
       " 1                NaN  \n",
       " 2                NaN  \n",
       " 3                NaN  \n",
       " 4                NaN  \n",
       " 5                NaN  \n",
       " 6                NaN  \n",
       " 7                NaN  \n",
       " 8                NaN  \n",
       " 9                NaN  \n",
       " 10               NaN  \n",
       " 11               NaN  \n",
       " 12               NaN  \n",
       " 13               NaN  \n",
       " 14               NaN  \n",
       " 15               NaN  \n",
       " 16               NaN  \n",
       " 17               NaN  \n",
       " 18               NaN  \n",
       " 19               NaN  \n",
       " 20               NaN  \n",
       " 21               NaN  \n",
       " 22               NaN  \n",
       " 23               NaN  \n",
       " 24               NaN  \n",
       " 25           2.53072  \n",
       " 26               NaN  \n",
       " 27               NaN  \n",
       " 28               NaN  \n",
       " 29               NaN  \n",
       " 30               NaN  \n",
       " 31               NaN  \n",
       " 32               NaN  \n",
       " 33               NaN  \n",
       " 34               NaN  \n",
       " 35               NaN  \n",
       " 36               NaN  \n",
       " 37               NaN  \n",
       " 38               NaN  \n",
       " 39               NaN  \n",
       " 40               NaN  \n",
       " 41               NaN  \n",
       " 42               NaN  \n",
       " 43               NaN  \n",
       " 44               NaN  \n",
       " 45               NaN  \n",
       " 46               NaN  \n",
       " 47               NaN  \n",
       " 48               NaN  \n",
       " 49               NaN  \n",
       " 50               NaN  \n",
       " 51           1.76056  ,\n",
       " 'SGD_dp_batch_size_8192_4gpus':     trainer/global_step  _step  _runtime  train_loss_step  epoch  _timestamp  \\\n",
       " 0                     0      0        40         2.390903      0  1648372568   \n",
       " 1                     1      1        41         2.618182      0  1648372569   \n",
       " 2                     2      2        41         2.409632      0  1648372569   \n",
       " 3                     3      3        42         2.382266      0  1648372570   \n",
       " 4                     4      4        42         2.854592      0  1648372570   \n",
       " 5                     5      5        44         4.384065      0  1648372572   \n",
       " 6                     6      6        46         4.012147      0  1648372574   \n",
       " 7                     6      7        46              NaN      1  1648372574   \n",
       " 8                     7      8        51         3.435173      1  1648372579   \n",
       " 9                     8      9        52         3.664042      1  1648372580   \n",
       " 10                    9     10        54         3.144501      1  1648372582   \n",
       " 11                   10     11        54         3.195889      1  1648372582   \n",
       " 12                   11     12        57         3.081101      1  1648372585   \n",
       " 13                   12     13        58         2.827945      1  1648372586   \n",
       " 14                   13     14        58         2.607582      1  1648372586   \n",
       " 15                   13     15        58              NaN      2  1648372586   \n",
       " \n",
       "     train_loss_epoch  \n",
       " 0                NaN  \n",
       " 1                NaN  \n",
       " 2                NaN  \n",
       " 3                NaN  \n",
       " 4                NaN  \n",
       " 5                NaN  \n",
       " 6                NaN  \n",
       " 7           2.660240  \n",
       " 8                NaN  \n",
       " 9                NaN  \n",
       " 10               NaN  \n",
       " 11               NaN  \n",
       " 12               NaN  \n",
       " 13               NaN  \n",
       " 14               NaN  \n",
       " 15          3.263868  ,\n",
       " 'SGD_dp_batch_size_2048_4gpus':     trainer/global_step  _step  _runtime  train_loss_step  epoch  _timestamp  \\\n",
       " 0                     0      0        35         2.382205      0  1648372482   \n",
       " 1                     1      1        36         2.400140      0  1648372483   \n",
       " 2                     2      2        36         2.707777      0  1648372483   \n",
       " 3                     3      3        36         3.673821      0  1648372483   \n",
       " 4                     4      4        36         4.163005      0  1648372483   \n",
       " 5                     5      5        36         5.222578      0  1648372483   \n",
       " 6                     6      6        37         4.147680      0  1648372484   \n",
       " 7                     7      7        37         3.306381      0  1648372484   \n",
       " 8                     8      8        38         4.035182      0  1648372485   \n",
       " 9                     9      9        38         2.886489      0  1648372485   \n",
       " 10                   10     10        39         2.878854      0  1648372486   \n",
       " 11                   11     11        39         3.682711      0  1648372486   \n",
       " 12                   12     12        39         3.405432      0  1648372486   \n",
       " 13                   13     13        40         3.407504      0  1648372487   \n",
       " 14                   14     14        40         3.077529      0  1648372487   \n",
       " 15                   15     15        40         3.169637      0  1648372487   \n",
       " 16                   16     16        41         2.683454      0  1648372488   \n",
       " 17                   17     17        41         2.308244      0  1648372488   \n",
       " 18                   18     18        42         2.707833      0  1648372489   \n",
       " 19                   19     19        42         2.655234      0  1648372489   \n",
       " 20                   20     20        43         2.560152      0  1648372490   \n",
       " 21                   21     21        43         2.510918      0  1648372490   \n",
       " 22                   22     22        44         2.317803      0  1648372491   \n",
       " 23                   23     23        44         2.223610      0  1648372491   \n",
       " 24                   24     24        45         2.186810      0  1648372492   \n",
       " 25                   24     25        46              NaN      1  1648372493   \n",
       " 26                   25     26        48         2.244568      1  1648372495   \n",
       " 27                   26     27        48         2.193285      1  1648372495   \n",
       " 28                   27     28        48         2.009049      1  1648372495   \n",
       " 29                   28     29        49         2.121168      1  1648372496   \n",
       " 30                   29     30        49         2.085999      1  1648372496   \n",
       " 31                   30     31        49         2.170903      1  1648372496   \n",
       " 32                   31     32        50         1.948614      1  1648372497   \n",
       " 33                   32     33        50         2.158827      1  1648372497   \n",
       " 34                   33     34        51         2.151595      1  1648372498   \n",
       " 35                   34     35        51         2.051746      1  1648372498   \n",
       " 36                   35     36        52         1.952415      1  1648372499   \n",
       " 37                   36     37        52         2.027159      1  1648372499   \n",
       " 38                   37     38        53         1.966849      1  1648372500   \n",
       " 39                   38     39        53         2.004494      1  1648372500   \n",
       " 40                   39     40        53         2.024491      1  1648372500   \n",
       " 41                   40     41        53         1.961847      1  1648372500   \n",
       " 42                   41     42        54         1.976081      1  1648372501   \n",
       " 43                   42     43        54         1.901197      1  1648372501   \n",
       " 44                   43     44        55         1.969630      1  1648372502   \n",
       " 45                   44     45        55         1.940805      1  1648372502   \n",
       " 46                   45     46        56         1.964961      1  1648372503   \n",
       " 47                   46     47        56         1.898335      1  1648372503   \n",
       " 48                   47     48        57         1.903053      1  1648372504   \n",
       " 49                   48     49        57         1.933724      1  1648372504   \n",
       " 50                   49     50        57         1.761307      1  1648372504   \n",
       " 51                   49     51        57              NaN      2  1648372504   \n",
       " \n",
       "     train_loss_epoch  \n",
       " 0                NaN  \n",
       " 1                NaN  \n",
       " 2                NaN  \n",
       " 3                NaN  \n",
       " 4                NaN  \n",
       " 5                NaN  \n",
       " 6                NaN  \n",
       " 7                NaN  \n",
       " 8                NaN  \n",
       " 9                NaN  \n",
       " 10               NaN  \n",
       " 11               NaN  \n",
       " 12               NaN  \n",
       " 13               NaN  \n",
       " 14               NaN  \n",
       " 15               NaN  \n",
       " 16               NaN  \n",
       " 17               NaN  \n",
       " 18               NaN  \n",
       " 19               NaN  \n",
       " 20               NaN  \n",
       " 21               NaN  \n",
       " 22               NaN  \n",
       " 23               NaN  \n",
       " 24               NaN  \n",
       " 25          3.039449  \n",
       " 26               NaN  \n",
       " 27               NaN  \n",
       " 28               NaN  \n",
       " 29               NaN  \n",
       " 30               NaN  \n",
       " 31               NaN  \n",
       " 32               NaN  \n",
       " 33               NaN  \n",
       " 34               NaN  \n",
       " 35               NaN  \n",
       " 36               NaN  \n",
       " 37               NaN  \n",
       " 38               NaN  \n",
       " 39               NaN  \n",
       " 40               NaN  \n",
       " 41               NaN  \n",
       " 42               NaN  \n",
       " 43               NaN  \n",
       " 44               NaN  \n",
       " 45               NaN  \n",
       " 46               NaN  \n",
       " 47               NaN  \n",
       " 48               NaN  \n",
       " 49               NaN  \n",
       " 50               NaN  \n",
       " 51          1.963216  ,\n",
       " 'SGD_dp_batch_size_512_4gpus':      trainer/global_step  _step  _runtime  train_loss_step  epoch  _timestamp  \\\n",
       " 0                      0      0        30         2.447535      0  1648372405   \n",
       " 1                      1      1        30         2.636137      0  1648372405   \n",
       " 2                      2      2        31         3.141139      0  1648372406   \n",
       " 3                      3      3        31         4.203696      0  1648372406   \n",
       " 4                      4      4        31         4.988063      0  1648372406   \n",
       " ..                   ...    ...       ...              ...    ...         ...   \n",
       " 193                  192    193        51         1.580170      1  1648372426   \n",
       " 194                  193    194        51         1.535819      1  1648372426   \n",
       " 195                  194    195        52         1.507741      1  1648372427   \n",
       " 196                  195    196        52         1.734261      1  1648372427   \n",
       " 197                  195    197        52              NaN      2  1648372427   \n",
       " \n",
       "      train_loss_epoch  \n",
       " 0                 NaN  \n",
       " 1                 NaN  \n",
       " 2                 NaN  \n",
       " 3                 NaN  \n",
       " 4                 NaN  \n",
       " ..                ...  \n",
       " 193               NaN  \n",
       " 194               NaN  \n",
       " 195               NaN  \n",
       " 196               NaN  \n",
       " 197           1.64051  \n",
       " \n",
       " [198 rows x 7 columns],\n",
       " 'SGD_dp_batch_size_128_4gpus':      trainer/global_step  _step  _runtime  train_loss_step  epoch  _timestamp  \\\n",
       " 0                      0      0        34         2.454476      0  1648372290   \n",
       " 1                      1      1        34         3.187948      0  1648372290   \n",
       " 2                      2      2        34         3.783150      0  1648372290   \n",
       " 3                      3      3        34         4.302678      0  1648372290   \n",
       " 4                      4      4        34         8.475398      0  1648372290   \n",
       " ..                   ...    ...       ...              ...    ...         ...   \n",
       " 517                  774    775        94         1.817231      1  1648372350   \n",
       " 518                  775    776        94         1.216002      1  1648372350   \n",
       " 519                  779    780        95         1.199796      1  1648372351   \n",
       " 520                  780    781        95         1.388824      1  1648372351   \n",
       " 521                  781    783        95              NaN      2  1648372351   \n",
       " \n",
       "      train_loss_epoch  \n",
       " 0                 NaN  \n",
       " 1                 NaN  \n",
       " 2                 NaN  \n",
       " 3                 NaN  \n",
       " 4                 NaN  \n",
       " ..                ...  \n",
       " 517               NaN  \n",
       " 518               NaN  \n",
       " 519               NaN  \n",
       " 520               NaN  \n",
       " 521          1.463055  \n",
       " \n",
       " [522 rows x 7 columns],\n",
       " 'SGD_dp_batch_size_32_4gpus':      trainer/global_step  _step  _runtime  train_loss_step  epoch  _timestamp\n",
       " 0                     12     12        38         4.564947      0  1648371922\n",
       " 1                     44     44        40         2.453320      0  1648371924\n",
       " 2                     46     46        40         2.263949      0  1648371924\n",
       " 3                     54     54        41         2.240820      0  1648371925\n",
       " 4                     66     66        42         2.397463      0  1648371926\n",
       " ..                   ...    ...       ...              ...    ...         ...\n",
       " 485                 3089   3090       278         1.161531      1  1648372162\n",
       " 486                 3093   3094       278         0.834570      1  1648372162\n",
       " 487                 3098   3099       279         0.499158      1  1648372163\n",
       " 488                 3114   3115       280         2.188230      1  1648372164\n",
       " 489                 3118   3119       280         0.931003      1  1648372164\n",
       " \n",
       " [490 rows x 6 columns],\n",
       " 'SGD_dp_batch_size_8192_1gpus': Empty DataFrame\n",
       " Columns: []\n",
       " Index: [],\n",
       " 'SGD_dp_batch_size_2048_1gpus':     trainer/global_step  _step  _runtime  train_loss_step  epoch  _timestamp  \\\n",
       " 0                     0      0        23         2.378599      0  1648371712   \n",
       " 1                     1      1        24         2.419253      0  1648371713   \n",
       " 2                     2      2        24         2.778703      0  1648371713   \n",
       " 3                     3      3        25         3.354625      0  1648371714   \n",
       " 4                     4      4        25         4.705425      0  1648371714   \n",
       " 5                     5      5        26         5.723037      0  1648371715   \n",
       " 6                     6      6        26         6.090533      0  1648371715   \n",
       " 7                     7      7        27         6.604627      0  1648371716   \n",
       " 8                     8      8        27         8.788239      0  1648371716   \n",
       " 9                     9      9        28         8.495030      0  1648371717   \n",
       " 10                   10     10        28         7.792514      0  1648371717   \n",
       " 11                   11     11        29         6.351874      0  1648371718   \n",
       " 12                   12     12        29         4.802438      0  1648371718   \n",
       " 13                   13     13        30         3.963925      0  1648371719   \n",
       " 14                   14     14        30         6.134727      0  1648371719   \n",
       " 15                   15     15        31         5.324747      0  1648371720   \n",
       " 16                   16     16        31         4.129182      0  1648371720   \n",
       " 17                   17     17        32         3.113030      0  1648371721   \n",
       " 18                   18     18        32         3.248461      0  1648371721   \n",
       " 19                   19     19        32         2.751126      0  1648371721   \n",
       " 20                   20     20        33         2.790308      0  1648371722   \n",
       " 21                   21     21        33         2.772371      0  1648371722   \n",
       " 22                   22     22        34         2.586316      0  1648371723   \n",
       " 23                   23     23        34         2.596285      0  1648371723   \n",
       " 24                   24     24        36         2.469417      0  1648371725   \n",
       " 25                   24     25        36              NaN      1  1648371725   \n",
       " 26                   25     26        38         2.522918      1  1648371727   \n",
       " 27                   26     27        39         2.351709      1  1648371728   \n",
       " 28                   27     28        39         2.585978      1  1648371728   \n",
       " 29                   28     29        40         2.398835      1  1648371729   \n",
       " 30                   29     30        40         2.346667      1  1648371729   \n",
       " 31                   30     31        41         2.326970      1  1648371730   \n",
       " 32                   31     32        41         2.376224      1  1648371730   \n",
       " 33                   32     33        41         2.290313      1  1648371730   \n",
       " 34                   33     34        42         2.323020      1  1648371731   \n",
       " 35                   34     35        42         2.267548      1  1648371731   \n",
       " 36                   35     36        43         2.282800      1  1648371732   \n",
       " 37                   36     37        43         2.251382      1  1648371732   \n",
       " 38                   37     38        44         2.217711      1  1648371733   \n",
       " 39                   38     39        44         2.229835      1  1648371733   \n",
       " 40                   39     40        45         2.261669      1  1648371734   \n",
       " 41                   40     41        45         2.194469      1  1648371734   \n",
       " 42                   41     42        46         2.201263      1  1648371735   \n",
       " 43                   42     43        46         2.248072      1  1648371735   \n",
       " 44                   43     44        47         2.173002      1  1648371736   \n",
       " 45                   44     45        47         2.177445      1  1648371736   \n",
       " 46                   45     46        48         2.208236      1  1648371737   \n",
       " 47                   46     47        48         2.183602      1  1648371737   \n",
       " 48                   47     48        49         2.229931      1  1648371738   \n",
       " 49                   48     49        49         2.221301      1  1648371738   \n",
       " 50                   49     50        49         2.225251      1  1648371738   \n",
       " 51                   49     51        50              NaN      2  1648371739   \n",
       " \n",
       "     train_loss_epoch  \n",
       " 0                NaN  \n",
       " 1                NaN  \n",
       " 2                NaN  \n",
       " 3                NaN  \n",
       " 4                NaN  \n",
       " 5                NaN  \n",
       " 6                NaN  \n",
       " 7                NaN  \n",
       " 8                NaN  \n",
       " 9                NaN  \n",
       " 10               NaN  \n",
       " 11               NaN  \n",
       " 12               NaN  \n",
       " 13               NaN  \n",
       " 14               NaN  \n",
       " 15               NaN  \n",
       " 16               NaN  \n",
       " 17               NaN  \n",
       " 18               NaN  \n",
       " 19               NaN  \n",
       " 20               NaN  \n",
       " 21               NaN  \n",
       " 22               NaN  \n",
       " 23               NaN  \n",
       " 24               NaN  \n",
       " 25          4.535004  \n",
       " 26               NaN  \n",
       " 27               NaN  \n",
       " 28               NaN  \n",
       " 29               NaN  \n",
       " 30               NaN  \n",
       " 31               NaN  \n",
       " 32               NaN  \n",
       " 33               NaN  \n",
       " 34               NaN  \n",
       " 35               NaN  \n",
       " 36               NaN  \n",
       " 37               NaN  \n",
       " 38               NaN  \n",
       " 39               NaN  \n",
       " 40               NaN  \n",
       " 41               NaN  \n",
       " 42               NaN  \n",
       " 43               NaN  \n",
       " 44               NaN  \n",
       " 45               NaN  \n",
       " 46               NaN  \n",
       " 47               NaN  \n",
       " 48               NaN  \n",
       " 49               NaN  \n",
       " 50               NaN  \n",
       " 51          2.285252  ,\n",
       " 'SGD_dp_batch_size_512_1gpus':      trainer/global_step  _step  _runtime  train_loss_step  epoch  _timestamp  \\\n",
       " 0                      0      0        21         2.410742      0  1648371678   \n",
       " 1                      1      1        21         2.752490      0  1648371678   \n",
       " 2                      2      2        21         2.737628      0  1648371678   \n",
       " 3                      3      3        21         3.834259      0  1648371678   \n",
       " 4                      4      4        21         5.095662      0  1648371678   \n",
       " ..                   ...    ...       ...              ...    ...         ...   \n",
       " 193                  192    193        46         1.473887      1  1648371703   \n",
       " 194                  193    194        47         1.604234      1  1648371704   \n",
       " 195                  194    195        47         1.577804      1  1648371704   \n",
       " 196                  195    196        47         1.381406      1  1648371704   \n",
       " 197                  195    197        47              NaN      2  1648371704   \n",
       " \n",
       "      train_loss_epoch  \n",
       " 0                 NaN  \n",
       " 1                 NaN  \n",
       " 2                 NaN  \n",
       " 3                 NaN  \n",
       " 4                 NaN  \n",
       " ..                ...  \n",
       " 193               NaN  \n",
       " 194               NaN  \n",
       " 195               NaN  \n",
       " 196               NaN  \n",
       " 197          1.619874  \n",
       " \n",
       " [198 rows x 7 columns],\n",
       " 'SGD_dp_batch_size_128_1gpus':      trainer/global_step  _step  _runtime  train_loss_step  epoch  _timestamp  \\\n",
       " 0                      1      1        21         3.322361      0  1648371640   \n",
       " 1                      4      4        21         5.161662      0  1648371640   \n",
       " 2                      5      5        21         5.269036      0  1648371640   \n",
       " 3                      9      9        22         3.224431      0  1648371641   \n",
       " 4                     10     10        22         2.732107      0  1648371641   \n",
       " ..                   ...    ...       ...              ...    ...         ...   \n",
       " 502                  776    777        54         1.222841      1  1648371673   \n",
       " 503                  777    778        54         1.118677      1  1648371673   \n",
       " 504                  778    779        54         1.203329      1  1648371673   \n",
       " 505                  780    781        54         1.263283      1  1648371673   \n",
       " 506                  781    783        54              NaN      2  1648371673   \n",
       " \n",
       "      train_loss_epoch  \n",
       " 0                 NaN  \n",
       " 1                 NaN  \n",
       " 2                 NaN  \n",
       " 3                 NaN  \n",
       " 4                 NaN  \n",
       " ..                ...  \n",
       " 502               NaN  \n",
       " 503               NaN  \n",
       " 504               NaN  \n",
       " 505               NaN  \n",
       " 506          1.468338  \n",
       " \n",
       " [507 rows x 7 columns],\n",
       " 'SGD_dp_batch_size_32_1gpus':      trainer/global_step  _step  _runtime  train_loss_step  epoch  _timestamp\n",
       " 0                     14     14        18         5.188556      0  1648371592\n",
       " 1                     17     17        18         3.954767      0  1648371592\n",
       " 2                     25     25        18         3.013579      0  1648371592\n",
       " 3                     35     35        18         3.005083      0  1648371592\n",
       " 4                     38     38        18         2.163107      0  1648371592\n",
       " ..                   ...    ...       ...              ...    ...         ...\n",
       " 491                 3086   3087        94         1.174098      1  1648371668\n",
       " 492                 3090   3091        95         1.645498      1  1648371669\n",
       " 493                 3097   3098        95         1.174476      1  1648371669\n",
       " 494                 3098   3099        95         1.003709      1  1648371669\n",
       " 495                 3105   3106        95         1.125607      1  1648371669\n",
       " \n",
       " [496 rows x 6 columns],\n",
       " 'SGD_dp_batch_size_8192_2gpus':     trainer/global_step  _step  _runtime  train_loss_step  epoch  _timestamp  \\\n",
       " 0                     0      0        39         2.343246      0  1648371067   \n",
       " 1                     1      1        40         2.417241      0  1648371068   \n",
       " 2                     2      2        41         2.485400      0  1648371069   \n",
       " 3                     3      3        42         2.576402      0  1648371070   \n",
       " 4                     4      4        43         3.028359      0  1648371071   \n",
       " 5                     5      5        44         3.302346      0  1648371072   \n",
       " 6                     6      6        47         4.070184      0  1648371075   \n",
       " 7                     6      7        47              NaN      1  1648371075   \n",
       " 8                     7      8        52         4.775397      1  1648371080   \n",
       " 9                     8      9        53         4.856526      1  1648371081   \n",
       " 10                    9     10        55         4.949286      1  1648371083   \n",
       " 11                   10     11        56         4.184242      1  1648371084   \n",
       " 12                   11     12        59         3.392799      1  1648371087   \n",
       " 13                   12     13        60         2.816032      1  1648371088   \n",
       " 14                   13     14        60         2.903746      1  1648371088   \n",
       " 15                   13     15        60              NaN      2  1648371088   \n",
       " \n",
       "     train_loss_epoch  \n",
       " 0                NaN  \n",
       " 1                NaN  \n",
       " 2                NaN  \n",
       " 3                NaN  \n",
       " 4                NaN  \n",
       " 5                NaN  \n",
       " 6                NaN  \n",
       " 7           2.721693  \n",
       " 8                NaN  \n",
       " 9                NaN  \n",
       " 10               NaN  \n",
       " 11               NaN  \n",
       " 12               NaN  \n",
       " 13               NaN  \n",
       " 14               NaN  \n",
       " 15          4.130736  ,\n",
       " 'SGD_dp_batch_size_128_2gpus':     trainer/global_step  _step  _runtime  train_loss_step  epoch  _timestamp  \\\n",
       " 0                    49      0        29         2.181488      0  1648370297   \n",
       " 1                    99      1        32         2.085283      0  1648370300   \n",
       " 2                   149      2        34         1.959760      0  1648370302   \n",
       " 3                   199      3        37         1.864811      0  1648370305   \n",
       " 4                   249      4        39         1.824337      0  1648370307   \n",
       " 5                   299      5        42         1.748104      0  1648370310   \n",
       " 6                   349      6        44         1.689489      0  1648370312   \n",
       " 7                   390      7        47              NaN      1  1648370315   \n",
       " 8                   399      8        48         1.787462      1  1648370316   \n",
       " 9                   449      9        50         1.464020      1  1648370318   \n",
       " 10                  499     10        53         1.419646      1  1648370321   \n",
       " 11                  549     11        55         1.453597      1  1648370323   \n",
       " 12                  599     12        58         1.254473      1  1648370326   \n",
       " 13                  649     13        60         1.219815      1  1648370328   \n",
       " 14                  699     14        63         1.319156      1  1648370331   \n",
       " 15                  749     15        65         1.492566      1  1648370333   \n",
       " 16                  781     16        67              NaN      2  1648370335   \n",
       " \n",
       "     train_loss_epoch  \n",
       " 0                NaN  \n",
       " 1                NaN  \n",
       " 2                NaN  \n",
       " 3                NaN  \n",
       " 4                NaN  \n",
       " 5                NaN  \n",
       " 6                NaN  \n",
       " 7           1.987961  \n",
       " 8                NaN  \n",
       " 9                NaN  \n",
       " 10               NaN  \n",
       " 11               NaN  \n",
       " 12               NaN  \n",
       " 13               NaN  \n",
       " 14               NaN  \n",
       " 15               NaN  \n",
       " 16          1.462412  ,\n",
       " 'SGD_dp_batch_size_32_2gpus':     trainer/global_step  _step  _runtime  train_loss_step  epoch  _timestamp  \\\n",
       " 0                    49      0        24         3.000532      0  1648370289   \n",
       " 1                    99      1        27         3.825331      0  1648370292   \n",
       " 2                   149      2        29         2.502269      0  1648370294   \n",
       " 3                   199      3        31         2.015135      0  1648370296   \n",
       " 4                   249      4        33         1.945419      0  1648370298   \n",
       " ..                  ...    ...       ...              ...    ...         ...   \n",
       " 59                 2949     59       153         1.278652      1  1648370418   \n",
       " 60                 2999     60       155         2.211522      1  1648370420   \n",
       " 61                 3049     61       157         1.409628      1  1648370422   \n",
       " 62                 3099     62       159         1.205655      1  1648370424   \n",
       " 63                 3125     63       161              NaN      2  1648370426   \n",
       " \n",
       "     train_loss_epoch  \n",
       " 0                NaN  \n",
       " 1                NaN  \n",
       " 2                NaN  \n",
       " 3                NaN  \n",
       " 4                NaN  \n",
       " ..               ...  \n",
       " 59               NaN  \n",
       " 60               NaN  \n",
       " 61               NaN  \n",
       " 62               NaN  \n",
       " 63           1.50301  \n",
       " \n",
       " [64 rows x 7 columns]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_info(log_name):\n",
    "    gpu_nums = log_name.split('_')[-1]\n",
    "    batch_size = log_name.split('_')[-2]\n",
    "    batch_size = int(batch_size)\n",
    "    gpu_nums = gpu_nums.replace('gpus','')\n",
    "    gpu_nums = int(gpu_nums)\n",
    "    return gpu_nums, batch_size\n",
    "\n",
    "def get_time_info(log_name):\n",
    "    data = res[log_name]\n",
    "    epoch_1_start_time = data[data[\"epoch\"] == 1].iloc[0][\"_timestamp\"]\n",
    "    epoch_1_end_time = data[data[\"epoch\"] == 1].iloc[-1][\"_timestamp\"]\n",
    "    t = epoch_1_end_time - epoch_1_start_time\n",
    "    return t\n",
    "get_time_info(\"SGD_dp_batch_size_8192_4gpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_size = 8192 can not work in my gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['gpu_nums', 'batch_size', 'time'])\n",
    "for key in res.keys():\n",
    "    # print(key)\n",
    "    if \"8192\" in key:\n",
    "        continue\n",
    "    gpu_nums, batch_size = get_info(key)\n",
    "    t = get_time_info(key)\n",
    "    df.loc[len(df)] = [gpu_nums, batch_size, t]\n",
    "df.sort_values(by=['gpu_nums','batch_size'], ascending=True, inplace=True)\n",
    "df[\"gpu_nums\"] = df[\"gpu_nums\"].astype(int)\n",
    "df[\"batch_size\"] = df[\"batch_size\"].astype(int)\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_markdown(\"problem2/2_1_table_src.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpu_nums</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2048</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>2048</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>2048</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    gpu_nums  batch_size   time\n",
       "0          1          32   38.0\n",
       "1          1         128   16.0\n",
       "2          1         512   13.0\n",
       "3          1        2048   13.0\n",
       "4          2          32   67.0\n",
       "5          2         128   18.0\n",
       "6          2         512    9.0\n",
       "7          2        2048   10.0\n",
       "8          4          32  121.0\n",
       "9          4         128   30.0\n",
       "10         4         512   11.0\n",
       "11         4        2048   11.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_768624/1156332872.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  t[\"speedup\"] = t[\"time\"].iloc[0]/t[\"time\"] * t[\"gpu_nums\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpu_nums</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>time</th>\n",
       "      <th>speedup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1.134328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>121.0</td>\n",
       "      <td>1.256198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gpu_nums  batch_size   time   speedup\n",
       "0         1          32   38.0  1.000000\n",
       "4         2          32   67.0  1.134328\n",
       "8         4          32  121.0  1.256198"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = df[df[\"batch_size\"] == 32]\n",
    "t[\"speedup\"] = t[\"time\"].iloc[0]/t[\"time\"] \n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_768624/3464596280.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  t[\"speedup\"] = t[\"time\"].iloc[0]/t[\"time\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpu_nums</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>time</th>\n",
       "      <th>speedup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gpu_nums  batch_size  time   speedup\n",
       "1         1         128  16.0  1.000000\n",
       "5         2         128  18.0  0.888889\n",
       "9         4         128  30.0  0.533333"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = df[df[\"batch_size\"] == 128]\n",
    "t[\"speedup\"] = t[\"time\"].iloc[0]/t[\"time\"] \n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_768624/3178531423.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  t[\"speedup\"] = t[\"time\"].iloc[0]/t[\"time\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpu_nums</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>time</th>\n",
       "      <th>speedup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.181818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    gpu_nums  batch_size  time   speedup\n",
       "2          1         512  13.0  1.000000\n",
       "6          2         512   9.0  1.444444\n",
       "10         4         512  11.0  1.181818"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = df[df[\"batch_size\"] == 512]\n",
    "t[\"speedup\"] = t[\"time\"].iloc[0]/t[\"time\"]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_768624/2197254454.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  t[\"speedup\"] = t[\"time\"].iloc[0]/t[\"time\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpu_nums</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>time</th>\n",
       "      <th>speedup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2048</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>2048</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>2048</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.181818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    gpu_nums  batch_size  time   speedup\n",
       "3          1        2048  13.0  1.000000\n",
       "7          2        2048  10.0  1.300000\n",
       "11         4        2048  11.0  1.181818"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = df[df[\"batch_size\"] == 2048]\n",
    "t[\"speedup\"] = t[\"time\"].iloc[0]/t[\"time\"]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_768624/2183533066.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  t[\"speedup\"] = t[\"time\"].iloc[0]/t[\"time\"]\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "for b in [32, 128, 512, 2048]:\n",
    "    t = df[df[\"batch_size\"] == b]\n",
    "    t[\"speedup\"] = t[\"time\"].iloc[0]/t[\"time\"]\n",
    "    t.to_markdown(f\"problem2/2_1_table_src_{b}.md\")\n",
    "    l.append(t)\n",
    "l = pd.concat(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpu_nums</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>time</th>\n",
       "      <th>speedup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.567164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>121.0</td>\n",
       "      <td>0.314050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2048</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>2048</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>2048</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.181818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    gpu_nums  batch_size   time   speedup\n",
       "0          1          32   38.0  1.000000\n",
       "4          2          32   67.0  0.567164\n",
       "8          4          32  121.0  0.314050\n",
       "1          1         128   16.0  1.000000\n",
       "5          2         128   18.0  0.888889\n",
       "9          4         128   30.0  0.533333\n",
       "2          1         512   13.0  1.000000\n",
       "6          2         512    9.0  1.444444\n",
       "10         4         512   11.0  1.181818\n",
       "3          1        2048   13.0  1.000000\n",
       "7          2        2048   10.0  1.300000\n",
       "11         4        2048   11.0  1.181818"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpu_nums</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>time</th>\n",
       "      <th>speedup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2048</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>2048</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>2048</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.181818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    gpu_nums  batch_size  time   speedup\n",
       "3          1        2048  13.0  1.000000\n",
       "7          2        2048  10.0  1.300000\n",
       "11         4        2048  11.0  1.181818"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[l[\"batch_size\"] == b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpu_nums</th>\n",
       "      <th>batch_32_time</th>\n",
       "      <th>batch_32_speedup</th>\n",
       "      <th>batch_128_time</th>\n",
       "      <th>batch_128_speedup</th>\n",
       "      <th>batch_512_time</th>\n",
       "      <th>batch_512_speedup</th>\n",
       "      <th>batch_2048_time</th>\n",
       "      <th>batch_2048_speedup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.567164</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.444444</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>121.0</td>\n",
       "      <td>0.314050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.181818</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.181818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gpu_nums  batch_32_time  batch_32_speedup  batch_128_time  \\\n",
       "0        1           38.0          1.000000            16.0   \n",
       "1        2           67.0          0.567164            18.0   \n",
       "2        4          121.0          0.314050            30.0   \n",
       "\n",
       "   batch_128_speedup  batch_512_time  batch_512_speedup  batch_2048_time  \\\n",
       "0           1.000000            13.0           1.000000             13.0   \n",
       "1           0.888889             9.0           1.444444             10.0   \n",
       "2           0.533333            11.0           1.181818             11.0   \n",
       "\n",
       "   batch_2048_speedup  \n",
       "0            1.000000  \n",
       "1            1.300000  \n",
       "2            1.181818  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.set_option('max_columns',1000)\n",
    "cols = []\n",
    "cols.append(\"gpu_nums\")\n",
    "for b in [32, 128, 512, 2048]:\n",
    "    cols.append(f\"batch_{b}_time\")\n",
    "    cols.append(f\"batch_{b}_speedup\")\n",
    "# print(cols)\n",
    "df = pd.DataFrame(columns=cols)\n",
    "for g in [1, 2, 4]:\n",
    "    temp_df = pd.DataFrame(columns=cols)\n",
    "    temp_df[\"gpu_nums\"] = [g]\n",
    "    for b in [32, 128, 512, 2048]:\n",
    "        t = l[l[\"batch_size\"] == b]\n",
    "        t = t[t[\"gpu_nums\"] == g]\n",
    "        temp_df[f\"batch_{b}_time\"] = t[\"time\"].values\n",
    "        temp_df[f\"batch_{b}_speedup\"] = t[\"speedup\"].values\n",
    "    df = pd.concat([df, temp_df])\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_markdown(\"problem2/2_1_table_src_all.md\")\n",
    "df.to_csv(\"problem2/2_1_table_src_all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That speed up we use T = epoch_time, **T(1) / T(N)**. \n",
    "\n",
    "According to the [campuswire](https://campuswire.com/c/G84B0471C/feed/127), we are suggested to use N * T(1) / T(N). But the not sure the T definition here. \n",
    "\n",
    "To clarify, we use t = batch_time and T = epoch_time.\n",
    "\n",
    "We can calculate using n * t(1) / t(N) if we would like to use the same problem size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speedup = T(1) / T(N) = n * t(1) / t(N),   T is the epoch time, t is the batch time.\n",
    "\n",
    "|    |   gpu_nums |   batch_32_time |   batch_32_speedup |   batch_128_time |   batch_128_speedup |   batch_512_time |   batch_512_speedup |   batch_2048_time |   batch_2048_speedup |\n",
    "|---:|-----------:|----------------:|-------------------:|-----------------:|--------------------:|-----------------:|--------------------:|------------------:|---------------------:|\n",
    "|  0 |          1 |              38 |           1        |               16 |            1        |               13 |             1       |                13 |              1       |\n",
    "|  1 |          2 |              67 |           0.567164 |               18 |            0.888889 |                9 |             1.44444 |                10 |              1.3     |\n",
    "|  2 |          4 |             121 |           0.31405  |               30 |            0.533333 |               11 |             1.18182 |                11 |              1.18182 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the **N * T(1) / T(N)** definition, we can get the following result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpu_nums</th>\n",
       "      <th>batch_32_time</th>\n",
       "      <th>batch_32_speedup</th>\n",
       "      <th>batch_128_time</th>\n",
       "      <th>batch_128_speedup</th>\n",
       "      <th>batch_512_time</th>\n",
       "      <th>batch_512_speedup</th>\n",
       "      <th>batch_2048_time</th>\n",
       "      <th>batch_2048_speedup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1.134328</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.777778</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.888889</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>121.0</td>\n",
       "      <td>1.256198</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.133333</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.727273</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.727273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gpu_nums  batch_32_time batch_32_speedup  batch_128_time batch_128_speedup  \\\n",
       "0        1           38.0              1.0            16.0               1.0   \n",
       "1        2           67.0         1.134328            18.0          1.777778   \n",
       "2        4          121.0         1.256198            30.0          2.133333   \n",
       "\n",
       "   batch_512_time batch_512_speedup  batch_2048_time batch_2048_speedup  \n",
       "0            13.0               1.0             13.0                1.0  \n",
       "1             9.0          2.888889             10.0                2.6  \n",
       "2            11.0          4.727273             11.0           4.727273  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = df.copy(deep=True)\n",
    "\n",
    "for b in [32, 128, 512, 2048]:\n",
    "    new_df[f\"batch_{b}_speedup\"] = new_df[f\"batch_{b}_speedup\"] * new_df[\"gpu_nums\"]\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "Report for each batch size per gpu (i.e., 32, 128, 512 ...), how much time spent in computation (including CPU-GPU transferring and calculation) and how much time spent in communication in 2-GPU and 4-GPU case for one epoch. (hint You could use the training time reported in Question 1 to facilitate your calculation). (5) Expected Answer: First, describe how do you get the compute and communication time in each setup. Second, list compute and communication time in Table 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "Expected Answer: Table 1 records the training time and speedup for different batch size up to 4 GPUs. Comment on which type of scaling we are measuring: weak-scaling or strong-scaling? Comment on if the other type scaling was used speedup number will be better or worse than what you we are measuring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we follow the definition, T is the epoch time, and we use T(1) / T(N) to calculate the speedup, the we use the stong-scaling (Strong scaling concerns the speedup for a fixed problem size with respect to the number of processors, and is governed by Amdahl’s law) to measure the speedup, because we measure the epoch time, and the problem size for one epoch is fixed.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text { Speedup }= 1 /(s+p / N),\n",
    "\\end{equation}\n",
    "\n",
    "s is the serial part, p is the parallel part, N is the number of processors.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{Speedup}= T(1) / T(N)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "If we use the weak-scaling (Weak scaling concerns the speedup for a scaled problem size with respect to the number of processors, and is governed by Gustafson’s law.) to measure the speedup.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text { Speedup }=s+p * N\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\text { Speedup }=N * T(1) / T(N)\n",
    "\\end{equation}\n",
    "\n",
    "Considering the larger problem size will be more efficient for more processors, if we use the weak-scaling to measure the speedup, we will get a better speedup efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggested Definition\n",
    "\n",
    "If we use the **N * T(1) / T(N)** definition, T is the epoch time, then we are using the weak-scaling to measure the speedup, because we vary the problem size with respect to the number of processors.\n",
    "\n",
    "Then if we use another definition (strong scaling), the score will be worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    "Report for each batch size per gpu (i.e., 32, 128, 512 ...), how much time spent in computation (including CPU-GPU transferring and calculation) and how much time spent in communication in 2-GPU and 4-GPU case for one epoch. (hint You could use the training time reported in Question 1 to facilitate your calculation). (5) Expected Answer: First, describe how do you get the compute and communication time in each setup. Second, list compute and communication time in Table 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one gpu, we consider all the time is used for computation (including CPU-GPU transferring and calculation).\n",
    "\n",
    "For two gpus or four gpus, the iteration number for each gpu is reduced, thus the computation time is reduced half of the original time, the communication time is the difference between the actual time and the reduced computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"problem2/2_1_table_src_all.csv\")\n",
    "cal_col = []\n",
    "for b in [32, 128, 512, 2048]:\n",
    "    df.drop(columns=[f\"batch_{b}_speedup\"], inplace=True)\n",
    "    cal_col.append(f\"batch_{b}_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpu_nums</th>\n",
       "      <th>batch_32_time</th>\n",
       "      <th>batch_128_time</th>\n",
       "      <th>batch_512_time</th>\n",
       "      <th>batch_2048_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>67.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>121.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gpu_nums  batch_32_time  batch_128_time  batch_512_time  batch_2048_time\n",
       "0         1           38.0            16.0            13.0             13.0\n",
       "1         2           67.0            18.0             9.0             10.0\n",
       "2         4          121.0            30.0            11.0             11.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "communication_col = []\n",
    "calculation_col = []\n",
    "for b in [32, 128, 512, 2048]:\n",
    "    communication_col.append(f\"batch_{b}_communication_time\")\n",
    "    calculation_col.append(f\"batch_{b}_calculation_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"calculation_time\"\n",
    "communication_time = []\n",
    "calculation_time = []\n",
    "for g in [1, 2, 4]:\n",
    "    if g == 1:\n",
    "        temp_df = df[df[\"gpu_nums\"] == g]\n",
    "        temp_df = temp_df[cal_col].to_numpy()[0]\n",
    "        base_df = temp_df\n",
    "        communication_time.append(temp_df-temp_df)\n",
    "        calculation_time.append(base_df)\n",
    "        continue\n",
    "    temp_df = df[df[\"gpu_nums\"] == g]\n",
    "    temp_df = temp_df[cal_col].to_numpy()[0]\n",
    "    calculation_time.append(base_df / g)\n",
    "    res_df = temp_df - base_df / g\n",
    "    communication_time.append(res_df)\n",
    "df[communication_col] = communication_time\n",
    "df[calculation_col] = calculation_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpu_nums</th>\n",
       "      <th>batch_32_time</th>\n",
       "      <th>batch_32_speedup</th>\n",
       "      <th>batch_128_time</th>\n",
       "      <th>batch_128_speedup</th>\n",
       "      <th>batch_512_time</th>\n",
       "      <th>batch_512_speedup</th>\n",
       "      <th>batch_2048_time</th>\n",
       "      <th>batch_2048_speedup</th>\n",
       "      <th>batch_32_communication_time</th>\n",
       "      <th>batch_128_communication_time</th>\n",
       "      <th>batch_512_communication_time</th>\n",
       "      <th>batch_2048_communication_time</th>\n",
       "      <th>batch_32_calculation_time</th>\n",
       "      <th>batch_128_calculation_time</th>\n",
       "      <th>batch_512_calculation_time</th>\n",
       "      <th>batch_2048_calculation_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>38.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.00</td>\n",
       "      <td>13.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.567164</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.444444</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>48.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3.50</td>\n",
       "      <td>19.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.50</td>\n",
       "      <td>6.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>121.0</td>\n",
       "      <td>0.314050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.181818</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.181818</td>\n",
       "      <td>111.5</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.75</td>\n",
       "      <td>7.75</td>\n",
       "      <td>9.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gpu_nums  batch_32_time  batch_32_speedup  batch_128_time  \\\n",
       "0        1           38.0          1.000000            16.0   \n",
       "1        2           67.0          0.567164            18.0   \n",
       "2        4          121.0          0.314050            30.0   \n",
       "\n",
       "   batch_128_speedup  batch_512_time  batch_512_speedup  batch_2048_time  \\\n",
       "0           1.000000            13.0           1.000000             13.0   \n",
       "1           0.888889             9.0           1.444444             10.0   \n",
       "2           0.533333            11.0           1.181818             11.0   \n",
       "\n",
       "   batch_2048_speedup  batch_32_communication_time  \\\n",
       "0            1.000000                          0.0   \n",
       "1            1.300000                         48.0   \n",
       "2            1.181818                        111.5   \n",
       "\n",
       "   batch_128_communication_time  batch_512_communication_time  \\\n",
       "0                           0.0                          0.00   \n",
       "1                          10.0                          2.50   \n",
       "2                          26.0                          7.75   \n",
       "\n",
       "   batch_2048_communication_time  batch_32_calculation_time  \\\n",
       "0                           0.00                       38.0   \n",
       "1                           3.50                       19.0   \n",
       "2                           7.75                        9.5   \n",
       "\n",
       "   batch_128_calculation_time  batch_512_calculation_time  \\\n",
       "0                        16.0                       13.00   \n",
       "1                         8.0                        6.50   \n",
       "2                         4.0                        3.25   \n",
       "\n",
       "   batch_2048_calculation_time  \n",
       "0                        13.00  \n",
       "1                         6.50  \n",
       "2                         3.25  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./problem2/2_3.csv', index=False)\n",
    "df.to_markdown('./problem2/2_3.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpu_nums</th>\n",
       "      <th>batch_32_communication_time</th>\n",
       "      <th>batch_128_communication_time</th>\n",
       "      <th>batch_512_communication_time</th>\n",
       "      <th>batch_2048_communication_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>48.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>111.5</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.75</td>\n",
       "      <td>7.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gpu_nums  batch_32_communication_time  batch_128_communication_time  \\\n",
       "0        1                          0.0                           0.0   \n",
       "1        2                         48.0                          10.0   \n",
       "2        4                        111.5                          26.0   \n",
       "\n",
       "   batch_512_communication_time  batch_2048_communication_time  \n",
       "0                          0.00                           0.00  \n",
       "1                          2.50                           3.50  \n",
       "2                          7.75                           7.75  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_cols = [\"gpu_nums\"] + communication_col\n",
    "df[display_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpu_nums</th>\n",
       "      <th>batch_32_calculation_time</th>\n",
       "      <th>batch_128_calculation_time</th>\n",
       "      <th>batch_512_calculation_time</th>\n",
       "      <th>batch_2048_calculation_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.00</td>\n",
       "      <td>13.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.50</td>\n",
       "      <td>6.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>9.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gpu_nums  batch_32_calculation_time  batch_128_calculation_time  \\\n",
       "0        1                       38.0                        16.0   \n",
       "1        2                       19.0                         8.0   \n",
       "2        4                        9.5                         4.0   \n",
       "\n",
       "   batch_512_calculation_time  batch_2048_calculation_time  \n",
       "0                       13.00                        13.00  \n",
       "1                        6.50                         6.50  \n",
       "2                        3.25                         3.25  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_cols = [\"gpu_nums\"] + calculation_col\n",
    "df[display_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\n",
    "Assume PyTorch DP implements the all-reduce algorithm as discussed in the class (reference below), calculate communication bandwidth utilization for each multi-gpu/batch-size-per-gpu setup. (5) Expected Answer: First, list the formula to calculate how long does it take to finish an allreduce. Second, list the formula to calculate the bandwidth utilization. Third, list the calculated results in Table 3.\n",
    "\n",
    "References:\n",
    "\n",
    "• PyTorch Data Parallel, Available at https://pytorch.org/docs/stable/modules/torch/nn/parallel/data_parallel.html.\n",
    "\n",
    "• Bringing HPC Techniques to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "We calculate the all-reduce (ring-allreduce).\n",
    "\n",
    "P: number of processes \n",
    "\n",
    "N: total number of model parameters\n",
    "\n",
    "Scatter-reduce: Each process sends N/P amount of data to (P-1) learners, Total amount sent (per process): N(P-1)/P\n",
    "\n",
    "AllGather: Each process again sends N/P amount of data to (P-1) learners\n",
    "\n",
    "Total communication cost per process is 2N(P-1)/P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  | Batch-size-per-GPU 32 | Batch-size-per-GPU 128 | Batch-size-per-GPU 512 |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "|  | Bandwidth Utilization(GB/s) | Bandwidth Utilization(GB/s) | Bandwidth Utilization(GB/s) |\n",
    "| 2-GPU |  |  |  |\n",
    "| 4-GPU |  |  |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the ResNet18 architecture to calculate the bandwidth utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 11173962, 11.173962M\n"
     ]
    }
   ],
   "source": [
    "from models.resnet import ResNet18\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "model = ResNet18()\n",
    "\n",
    "# calculate model parameters size\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params_count = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(f\"Model parameters: {params_count}, {params_count/1e6}M\")\n",
    "\n",
    "N = params_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),#random crop\n",
    "                                      transforms.RandomHorizontalFlip(0.5),#random flip\n",
    "                                      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),#normalize\n",
    "                                      transforms.ToTensor(),])#convert to tensor\n",
    "train_dataset = datasets.CIFAR10(\"./cached_datasets/CIFAR10\", train=True, download=True, transform=train_transform)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "dataset_size = len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpu_nums</th>\n",
       "      <th>batch_32_time</th>\n",
       "      <th>batch_128_time</th>\n",
       "      <th>batch_512_time</th>\n",
       "      <th>batch_2048_time</th>\n",
       "      <th>batch_32_communication_time</th>\n",
       "      <th>batch_128_communication_time</th>\n",
       "      <th>batch_512_communication_time</th>\n",
       "      <th>batch_2048_communication_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>67.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>121.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>111.5</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.75</td>\n",
       "      <td>7.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gpu_nums  batch_32_time  batch_128_time  batch_512_time  batch_2048_time  \\\n",
       "0         1           38.0            16.0            13.0             13.0   \n",
       "1         2           67.0            18.0             9.0             10.0   \n",
       "2         4          121.0            30.0            11.0             11.0   \n",
       "\n",
       "   batch_32_communication_time  batch_128_communication_time  \\\n",
       "0                          0.0                           0.0   \n",
       "1                         48.0                          10.0   \n",
       "2                        111.5                          26.0   \n",
       "\n",
       "   batch_512_communication_time  batch_2048_communication_time  \n",
       "0                          0.00                           0.00  \n",
       "1                          2.50                           3.50  \n",
       "2                          7.75                           7.75  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidth_cols = []\n",
    "communication_count_cols = []\n",
    "for b in [32, 128, 512, 2048]:\n",
    "    bandwidth_cols.append(f\"batch_{b}_bandwidth\")\n",
    "    communication_count_cols.append(f\"batch_{b}_communication_count\")\n",
    "df[bandwidth_cols] = np.nan\n",
    "df[communication_count_cols] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_768624/3602963460.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"batch_\" + str(b) + \"_bandwidth\"][i] = bandwidth_utilization\n",
      "/tmp/ipykernel_768624/3602963460.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"batch_\" + str(b) + \"_communication_count\"][i] = communication_count\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "append_df = []\n",
    "# P = 1\n",
    "\n",
    "for i,P in enumerate([1, 2, 4]):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    for b in [32, 128, 512, 2048]:\n",
    "        iter_num = math.ceil(dataset_size/b)\n",
    "        each_iter_communication_count = 2 * N * (P - 1) / P\n",
    "        communication_count = iter_num * each_iter_communication_count\n",
    "        communication_time = df[\"batch_\" + str(b) + \"_communication_time\"][i]\n",
    "        # print(communication_time)\n",
    "        # print(f\"communication_count: {communication_count}\")\n",
    "        bandwidth_utilization = communication_count / (communication_time * 1e9)\n",
    "        df[\"batch_\" + str(b) + \"_bandwidth\"][i] = bandwidth_utilization\n",
    "        df[\"batch_\" + str(b) + \"_communication_count\"][i] = communication_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpu_nums</th>\n",
       "      <th>batch_32_time</th>\n",
       "      <th>batch_128_time</th>\n",
       "      <th>batch_512_time</th>\n",
       "      <th>batch_2048_time</th>\n",
       "      <th>batch_32_communication_time</th>\n",
       "      <th>batch_128_communication_time</th>\n",
       "      <th>batch_512_communication_time</th>\n",
       "      <th>batch_2048_communication_time</th>\n",
       "      <th>batch_32_bandwidth</th>\n",
       "      <th>batch_128_bandwidth</th>\n",
       "      <th>batch_512_bandwidth</th>\n",
       "      <th>batch_2048_bandwidth</th>\n",
       "      <th>batch_32_communication_count</th>\n",
       "      <th>batch_128_communication_count</th>\n",
       "      <th>batch_512_communication_count</th>\n",
       "      <th>batch_2048_communication_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>67.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.363852</td>\n",
       "      <td>0.436902</td>\n",
       "      <td>0.438019</td>\n",
       "      <td>0.079814</td>\n",
       "      <td>1.746490e+10</td>\n",
       "      <td>4.369019e+09</td>\n",
       "      <td>1.095048e+09</td>\n",
       "      <td>279349050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>121.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>111.5</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.75</td>\n",
       "      <td>7.75</td>\n",
       "      <td>0.234954</td>\n",
       "      <td>0.252059</td>\n",
       "      <td>0.211945</td>\n",
       "      <td>0.054068</td>\n",
       "      <td>2.619735e+10</td>\n",
       "      <td>6.553529e+09</td>\n",
       "      <td>1.642572e+09</td>\n",
       "      <td>419023575.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gpu_nums  batch_32_time  batch_128_time  batch_512_time  batch_2048_time  \\\n",
       "0         1           38.0            16.0            13.0             13.0   \n",
       "1         2           67.0            18.0             9.0             10.0   \n",
       "2         4          121.0            30.0            11.0             11.0   \n",
       "\n",
       "   batch_32_communication_time  batch_128_communication_time  \\\n",
       "0                          0.0                           0.0   \n",
       "1                         48.0                          10.0   \n",
       "2                        111.5                          26.0   \n",
       "\n",
       "   batch_512_communication_time  batch_2048_communication_time  \\\n",
       "0                          0.00                           0.00   \n",
       "1                          2.50                           3.50   \n",
       "2                          7.75                           7.75   \n",
       "\n",
       "   batch_32_bandwidth  batch_128_bandwidth  batch_512_bandwidth  \\\n",
       "0                 NaN                  NaN                  NaN   \n",
       "1            0.363852             0.436902             0.438019   \n",
       "2            0.234954             0.252059             0.211945   \n",
       "\n",
       "   batch_2048_bandwidth  batch_32_communication_count  \\\n",
       "0                   NaN                           NaN   \n",
       "1              0.079814                  1.746490e+10   \n",
       "2              0.054068                  2.619735e+10   \n",
       "\n",
       "   batch_128_communication_count  batch_512_communication_count  \\\n",
       "0                            NaN                            NaN   \n",
       "1                   4.369019e+09                   1.095048e+09   \n",
       "2                   6.553529e+09                   1.642572e+09   \n",
       "\n",
       "   batch_2048_communication_count  \n",
       "0                             NaN  \n",
       "1                     279349050.0  \n",
       "2                     419023575.0  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_32_communication_count</th>\n",
       "      <th>batch_128_communication_count</th>\n",
       "      <th>batch_512_communication_count</th>\n",
       "      <th>batch_2048_communication_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.746490e+10</td>\n",
       "      <td>4.369019e+09</td>\n",
       "      <td>1.095048e+09</td>\n",
       "      <td>279349050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.619735e+10</td>\n",
       "      <td>6.553529e+09</td>\n",
       "      <td>1.642572e+09</td>\n",
       "      <td>419023575.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch_32_communication_count  batch_128_communication_count  \\\n",
       "0                           NaN                            NaN   \n",
       "1                  1.746490e+10                   4.369019e+09   \n",
       "2                  2.619735e+10                   6.553529e+09   \n",
       "\n",
       "   batch_512_communication_count  batch_2048_communication_count  \n",
       "0                            NaN                             NaN  \n",
       "1                   1.095048e+09                     279349050.0  \n",
       "2                   1.642572e+09                     419023575.0  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[communication_count_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No communication for gpu_nums = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpu_nums</th>\n",
       "      <th>batch_32_bandwidth</th>\n",
       "      <th>batch_128_bandwidth</th>\n",
       "      <th>batch_512_bandwidth</th>\n",
       "      <th>batch_2048_bandwidth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.363852</td>\n",
       "      <td>0.436902</td>\n",
       "      <td>0.438019</td>\n",
       "      <td>0.079814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.234954</td>\n",
       "      <td>0.252059</td>\n",
       "      <td>0.211945</td>\n",
       "      <td>0.054068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gpu_nums  batch_32_bandwidth  batch_128_bandwidth  batch_512_bandwidth  \\\n",
       "0         1                 NaN                  NaN                  NaN   \n",
       "1         2            0.363852             0.436902             0.438019   \n",
       "2         4            0.234954             0.252059             0.211945   \n",
       "\n",
       "   batch_2048_bandwidth  \n",
       "0                   NaN  \n",
       "1              0.079814  \n",
       "2              0.054068  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_cols = [\"gpu_nums\"] + bandwidth_cols\n",
    "df[display_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bandwidth utilization is calculated by the following formula:\n",
    "1. Calculate the iteration numer\n",
    "2. Calculate the communication cost per process per iter.\n",
    "3. Calculate the communication cost per process per epoch.\n",
    "4. Using the epoch communication time to get the bandwidth utilization.\n",
    "\n",
    "The above table unit is GB/s.\n",
    "\n",
    "\\begin{align}\n",
    "T_{train} = T_{communication}+T_{compute} \\\\\n",
    "\n",
    "T_{compute} = T_1/N \\\\\n",
    "\n",
    "T_{communication} = T_{train} - T_1/N \n",
    "\\end{align}\n",
    "\n",
    "We have the T_{communication}, and we have all-reduce to get the model transfer size, we can get the bandwidth utilization by \n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{Bandwidth Utilization}= \\frac{Size_{communication}}{T_{communication}}\n",
    "\\end{equation}"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6af05ba0047cd36248220b2bd1e90eef247fc9b76c139bbd2c6e6e388383ae6c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
