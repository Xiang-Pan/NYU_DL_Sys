<!--
 * @Created by: Xiang Pan
 * @Date: 2022-04-27 01:42:07
 * @LastEditors: Xiang Pan
 * @LastEditTime: 2022-04-27 02:58:22
 * @Email: xiangpan@nyu.edu
 * @FilePath: /HW5/problem4/4_1.md
 * @Description: 
-->
# 4

## 1


Episodic And Non-Episodic Environment

<font color=Green>
Episodic: Non-Sequential Environment, current action will not affect the next action. In RL, episodes are considered agent-environment interactions from initial to final states. In a car racing video game, you start the game (initial state) and play the game until it is over (final state). This is called an episode

Continuous: Current action will affect the next action. When we play the poker game, current action will affect the next action.
</font>

## 2

What do the terms exploration and exploitation mean in RL ? Why do the actors employ ϵ-greedy policy for selecting actions at each step? Should ϵ remain fixed or follow a schedule during Deep RL training ? How does the value of ϵ help balance exploration and exploitation during training.

<font color=Green>

Exploration: explore the action space and get the reward corresponding to the action.

Exploitation: exploit the action space and choose the action with the highest Q value.

Epsilon-greedy policy: We takes action using the greedy policy with a probability of 1− $\epsilon$  and a random action with a probability of $\epsilon$. This approach ensures all the action space is explored.

$\epsilon$ should follow a schedule during training, but for burn-in period, we may use a fixed value.

Larger $\epsilon$ means more exploration, but less exploitation.

</font>

## 3

How is the Deep Q-Learning algorithm different from Q-learning ? You will follow the steps of Deep Q-Learning algorithm in Mnih et al. (2013) page 5, and explain each step in your own words. (2)

<font color=Green>
The biggest difference between DQN and Q learning is the Q table. In Q learning, this is a table. Enter (s, a) to query the corresponding Q value. In DQN, this is a function that is replaced by a neural network. Input (s, a) can output the corresponding Q value. 

**Update Steps**:

- $Q(s, a| \theta)$ is the Q function. 

- For current state, we have $Q = Q(s, a| \theta)$. We can use MC or TD to get the Q' as the target Q.

- Then we use Q' - Q as the loss to update the Q function.

</font>

## 4
What is the benefit of having a target Q-network?

<font color=Green>
Neural Network can be seens as a **compressed** version of the Q table, especially for infinite state space. Thus we can use the target Q-network to estimate the Q-value of the given state-action pair, which can be used to get the best action given the current state.
</font>

## 5

How does experience replay help in efficient Q-learning?

<font color=Green>
This means instead of running Q-learning on (state, action) pairs as they occur during simulation or actual experience, the system stores the data discovered as (state, action, reward, next_state).

The experience replay use the stored data to train the Q-network, which means the past experience can be sampled multiple times to train the Q-network.

</font>

## 6

What is prioritized experience replay and how is priority of a sample calculated?

<font color=Green>
Prioritized Experience Replay is a type of experience replay with  more frequently replay transitions with high expected learning progress, as measured by the magnitude of their temporal-difference (TD) error. 


For the sampling transition i,
$P(i)=\frac{p_{i}^{\alpha}}{\sum_{k} p_{k}^{\alpha}}$, $\alpha$ controls the priority of the transition.

</font>


## 7
Compare and contrast GORILA (General Reinforcement Learning Architecture) and Ape-X architecture. Provide three similarities and three differences. (3)

<font color=Green>

GORILA的四要素：Parameter Server、Learner、Actor、Replay Memory
Actor：Policy（此处为Q-Network）与环境交互，同样涉及探索与利用，而此处多了一个Bundled Mode即Actor的Policy与Learner中实时更新的Q-Network是捆绑的
Learner：Double Q-Network的结构、Loss与Gradient的计算
Replay Memory：在Actor与Learner之间接收样本与提供样本的样本池
Parameter Server：存储Q-Network中参数的Gradient变化，好处是可以让Q-Network进行回滚，并且可以通过多个Gradient（即历史信息）来使训练过程更加稳定。

**Similartity**
- Both of GORILA and Ape-X architecture have two parts: acting and learning.
- Both have a replay memory.
- Both have a parameter server.

**Differences**
- GORILA's Replay Buffer is off-line, Ape-X's Replay Buffer is on-line.(Replay Experiences)
- The replay memory is centralized in Ape-X, while GORILA can be distributed.
- Ape-X is not uniformly sampling (Ape-X use prioritized experience replay), but GORILA is uniformly sampling.

</font>


## 8
Why the performance improves with number of actors in Ape-X?


<font color=Green>
Actor is used to sample and generate experiences from the environment, more actors means more exhaustive sampling.
</font>